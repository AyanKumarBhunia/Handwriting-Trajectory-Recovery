{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12505697535180575894\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1507328\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 16988791617578994689\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import itertools\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input\n",
    "def load_Images(Data_Train_Path):\n",
    "    main_path = [f for f in np.sort(os.listdir(Data_Train_Path)) if f.endswith('redraw')]\n",
    "    height = 64\n",
    "    imgs = []\n",
    "    for s in main_path:\n",
    "        sub = Data_Train_Path + s\n",
    "        sub_path = [f for f in np.sort(os.listdir(sub)) if f.endswith('.tif')]\n",
    "        for t in sub_path:\n",
    "            im_path = sub + '/' + t           \n",
    "            img = cv2.imread(im_path,cv2.IMREAD_UNCHANGED)\n",
    "            imgs.append(img)            \n",
    "    return np.expand_dims(imgs,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input\n",
    "def load_Coordinates(Data_Train_Path):\n",
    "    Data_Label = []\n",
    "    DATA_Label_K = []\n",
    "    main_path = [f for f in np.sort(os.listdir(Data_Train_Path)) if f.endswith('online')]\n",
    "    for s in main_path:\n",
    "        sub = Data_Train_Path + s\n",
    "        sub_path = [f for f in np.sort(os.listdir(sub)) if f.endswith('.txt')]\n",
    "        for t in sub_path:\n",
    "            txt_path = sub + '/' + t            \n",
    "            file_B = open(txt_path, 'r') \n",
    "            data = file_B.read()\n",
    "            x = data.split('\\n')\n",
    "            online_data = np.zeros([50,2],np.int32)           \n",
    "            online_data_K = np.zeros([51,2],np.int32)\n",
    "            \n",
    "            for y in range(50):\n",
    "                z = x[y].split(\" \")\n",
    "                online_data[y] = [int(z[0]),int(z[1])]\n",
    "                online_data_K[y+1] = [int(z[0]),int(z[1])]\n",
    "            online_data_K = online_data_K[:50,:]\n",
    "            #print(online_data)\n",
    "            file_B.close() \n",
    "            Data_Label.append(online_data)\n",
    "            DATA_Label_K.append(online_data_K)\n",
    "    return np.array(Data_Label), np.array(DATA_Label_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Architecture(Input, outputs, is_training = True):\n",
    "    #Input,_,_ = load_patch_data(Data_Train_Path,height = 224,win_size = 224,win_step = 112,return_all = True)\n",
    "    #Input.shape           \n",
    "    with tf.variable_scope(\"Network\"):\n",
    "        \n",
    "        with tf.variable_scope(\"CNN\"):\n",
    "            conv1 = tf.layers.conv2d(Input,64,3,activation=tf.nn.relu,name = 'conv1', padding = 'same')\n",
    "            pool1 = tf.layers.max_pooling2d(conv1,2,[2,2],name = 'pool1')\n",
    "\n",
    "            conv2 = tf.layers.conv2d(pool1,128,3,activation=tf.nn.relu,name = 'conv2', padding = 'same')\n",
    "            pool2 = tf.layers.max_pooling2d(conv2, 2,[2,1],name = 'pool2',  padding = 'same')\n",
    "\n",
    "\n",
    "            conv3 = tf.layers.conv2d(pool2,256,3,activation=None,name = 'conv3', padding = 'same')\n",
    "            b_norm3 = tf.nn.relu(tf.layers.batch_normalization(conv3, training = is_training, name='batch-norm1'), name = 'relu3')\n",
    "\n",
    "\n",
    "            conv4 = tf.layers.conv2d(b_norm3,256,3,activation=tf.nn.relu,name = 'conv4', padding = 'same')\n",
    "            pool4 = tf.layers.max_pooling2d(conv4, 2,[2,1],name = 'pool4',  padding = 'same')\n",
    "\n",
    "            conv5 = tf.layers.conv2d(pool4,256,3,activation=tf.nn.relu,name = 'conv5', padding = 'same')\n",
    "            pool5 = tf.layers.max_pooling2d(conv5, 2,[2,1],name = 'pool5',  padding = 'same')\n",
    "\n",
    "            conv6 = tf.layers.conv2d(pool5,256,3,activation=None,name = 'conv6', padding = 'same')\n",
    "            b_norm6 = tf.nn.relu(tf.layers.batch_normalization(conv6, training = is_training, name='batch-norm2'), name = 'relu6')\n",
    "\n",
    "            conv7 = tf.layers.conv2d(b_norm6,256,3,activation=tf.nn.relu,name = 'conv7', padding = 'same')\n",
    "            pool7 = tf.layers.max_pooling2d(conv7, 2,[2,1],name = 'pool7',  padding = 'same')\n",
    "\n",
    "            conv8 = tf.layers.conv2d(pool7,256,2,activation=None, name = 'conv8')\n",
    "            b_norm8 = tf.nn.relu(tf.layers.batch_normalization(conv8, training = is_training, name='batch-norm3'), name = 'relu8')\n",
    "\n",
    "            shape = b_norm8.get_shape().as_list()\n",
    "            transposed = tf.transpose(b_norm8, perm=[0, 2, 1, 3], name='transposed') \n",
    "            conv_reshaped = tf.reshape(transposed, [ shape[0], -1, shape[1] * shape[3] ], name='reshaped') \n",
    "            \n",
    "        num_units = 256\n",
    "        num_layers = 3\n",
    "        \n",
    "        cell_stack_encoder = []\n",
    "        cell_stack_decoder = []\n",
    "\n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            for i in range(num_layers):\n",
    "                cell_stack_encoder.append(tf.contrib.rnn.LSTMCell(num_units))        \n",
    "            encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_stack_encoder, state_is_tuple=True)\n",
    "            _, encoder_state = tf.nn.dynamic_rnn(encoder_cell, conv_reshaped, dtype = tf.float32)\n",
    "\n",
    "        with tf.variable_scope(\"Decoder\"):\n",
    "            for i in range(num_layers):\n",
    "                cell_stack_decoder.append(tf.contrib.rnn.LSTMCell(num_units))  \n",
    "            decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_stack_decoder, state_is_tuple=True)\n",
    "            dec_outputs, _ = tf.nn.dynamic_rnn(decoder_cell, outputs, initial_state = encoder_state, dtype = tf.float32)    \n",
    "            \n",
    "            logits = tf.layers.dense(dec_outputs, 2, name  = 'logits')  \n",
    "        \n",
    "            return logits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "Input = tf.placeholder(tf.float32, shape = [BatchSize, 64, 64, 1])\n",
    "outputs = tf.placeholder(tf.float32, shape = [BatchSize, 64, 2])\n",
    "Architecture(Input, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss_function(Dec_Outputs, Targets):\n",
    "    loss = tf.reduce_mean(tf.abs(Dec_Outputs - Targets), name = 'loss')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss_function_Test(Dec_Outputs, Targets):\n",
    "    loss_ = np.sum(np.abs(Dec_Outputs - Targets))\n",
    "    return loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reDraw(dataTensor,predTensor, num, isTrain):\n",
    "    for x in range(num):        \n",
    "        imgData = np.zeros([64,64],np.int32)\n",
    "        imgPred = np.zeros([64,64],np.int32)\n",
    "\n",
    "        for y in range(50):        \n",
    "            if (dataTensor[x,y,0] < 64) and (dataTensor[x,y,1] < 64):\n",
    "                imgData[dataTensor[x,y,1],dataTensor[x,y,0]] = 1\n",
    "            if (predTensor[x,y,0] < 64) and (predTensor[x,y,1] < 64):\n",
    "                imgPred[predTensor[x,y,1],predTensor[x,y,0]] = 1\n",
    "\n",
    "        fig = plt.figure(figsize = (12,12))\n",
    "        fig.add_subplot(1,2,1)\n",
    "        plt.imshow(imgData)\n",
    "        plt.title('Ground Truth' + isTrain)\n",
    "\n",
    "        fig.add_subplot(1,2,2)\n",
    "        plt.imshow(imgPred)\n",
    "        plt.title('Predicted' + isTrain)   \n",
    "\n",
    "    #plt.tight_layout()    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up Saver...\")\n",
    "saver = tf.train.Saver()\n",
    "summary_writer = tf.summary.FileWriter('./logs_dir_TELEGU_Error/', sess.graph)\n",
    "ckpt = tf.train.get_checkpoint_state('./logs_dir_TELEGU_Error/')\n",
    "\n",
    "\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"Model restored...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compile(Data_Train,LabelTrain_Outputs, LabelTrain_Targets, BatchSize = 32, learning_rate = 0.001, num_epochs = 50):\n",
    "\n",
    "    #Data_Train,Train_Label,Train_seq = load_patch_data(params['trainpath'])\n",
    "    #Data_Test,Test_Label,Test_Seq = load_patch_data(params['valpath'])\n",
    "    [Total_Data, row, col, depth] = np.shape(Data_Train)\n",
    "    \n",
    "    \n",
    "    random_index = np.random.choice(Total_Data, 1000, replace=False)\n",
    "    \n",
    "    Data_Test = Data_Train[random_index,:,:,:]\n",
    "    LabelTest_Outputs = LabelTrain_Outputs[random_index, :,:]\n",
    "    LabelTest_Targets = LabelTrain_Targets[random_index, :,:]\n",
    "    \n",
    "    \n",
    "    Data_Train = np.delete(Data_Train,random_index, axis = 0)\n",
    "    LabelTrain_Outputs = np.delete(LabelTrain_Outputs,random_index, axis = 0)\n",
    "    LabelTrain_Targets = np.delete(LabelTrain_Targets,random_index, axis = 0)\n",
    "    \n",
    "    [Total_Data, row, col, depth] = np.shape(Data_Train)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    Inputs = tf.placeholder(tf.float32, shape = [BatchSize, 64, 64, 1], name = 'Inputs')\n",
    "    outputs = tf.placeholder(tf.float32, shape = [BatchSize, None, 2], name = 'outputs')\n",
    "    targets = tf.placeholder(tf.float32, shape = [BatchSize, 50, 2], name = 'targets')\n",
    "\n",
    "\n",
    "    logits = Architecture(Inputs, outputs)\n",
    "    loss = Loss_function(logits, targets)\n",
    "    \n",
    "     \n",
    "\n",
    "    training_step = tf.train.AdamOptimizer(learning_rate = 0.001, name='Adam').minimize(loss)\n",
    "\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    train_loss_list = []\n",
    "    mean_loss_list = []\n",
    "\n",
    "    timeperepoch = []\n",
    "\n",
    "    \n",
    "    tf.summary.scalar(\"TrainLoss\", loss)\n",
    "    tf.summary.histogram(\"X_Value\", logits[:,0])\n",
    "    tf.summary.histogram(\"Y_Value\", logits[:,1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #tf.summary.image(\"Target\", draw_Targ, max_outputs = 5)\n",
    "    #tf.summary.image(\"Pred\", draw_Pred, max_outputs = 5)\n",
    "    \n",
    "    \n",
    "    print(\"Setting up summary op...\")\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "    print(\"Setting up Saver...\")\n",
    "    saver = tf.train.Saver()\n",
    "    summary_writer = tf.summary.FileWriter('./logs_dir_TELEGU_Error/', sess.graph)\n",
    "    ckpt = tf.train.get_checkpoint_state('./logs_dir_TELEGU_Error/')\n",
    "    \n",
    "    \n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\"Model restored...\")\n",
    "    \n",
    "    itr = 0\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        index = np.random.permutation(Total_Data)\n",
    "        \n",
    "        Data_Train = Data_Train[index,:,:,:]\n",
    "        \n",
    "        LabelTrain_Targets = LabelTrain_Targets[index,:,:]\n",
    "        LabelTrain_Outputs = LabelTrain_Outputs[index,:,:]\n",
    "        \n",
    "        for k in range(Total_Data//BatchSize):         \n",
    "            start = time.time()\n",
    "            \n",
    "            batch_input = Data_Train[k*BatchSize:(k+1)*BatchSize,:,:,:]                      \n",
    "            batch_outputs = LabelTrain_Outputs[k*BatchSize:(k+1)*BatchSize,:,:]\n",
    "            batch_targets = LabelTrain_Targets[k*BatchSize:(k+1)*BatchSize,:,:] \n",
    "            train_feed_dict = {Inputs:batch_input, outputs:batch_outputs, targets:batch_targets}\n",
    "            \n",
    "            \n",
    "            _,train_loss,summary_str,logits_Out = sess.run([training_step,loss,summary_op,logits],train_feed_dict)\n",
    "            summary_writer.add_summary(summary_str, itr)\n",
    "            itr = itr + 1\n",
    "            \n",
    "            train_loss_list.append(train_loss)\n",
    "           \n",
    "            end = time.time()    \n",
    "             \n",
    "            timeperepoch.append(end-start)\n",
    "\n",
    "            #print('Epoch: '+str(i) + ' Step:' + str(k) + ' train loss :'+ str(train_loss) + '  Time per epoch : ' +str(timeperepoch[-1]) )\n",
    "            \n",
    "            \n",
    "            if k%300 == 0:\n",
    "                \n",
    "                index = np.random.permutation(1000)\n",
    "                Data_Test = Data_Test[index,:,:,:]     \n",
    "                LabelTest_Targets = LabelTest_Targets[index,:,:]\n",
    "                \n",
    "                \n",
    "                \n",
    "                Test_loss = []\n",
    "                BatchSize_Test = BatchSize\n",
    "                for jk in range(5): #(1000//BatchSize_Test):                             \n",
    "                    batch_Test_input = Data_Test[jk*BatchSize_Test:(jk+1)*BatchSize_Test,:,:,:]  \n",
    "                    batch_Test_targets = LabelTest_Targets[jk*BatchSize_Test:(jk+1)*BatchSize_Test,:,:]                         \n",
    "                    batch_Test_outputs =  np.zeros(shape = [BatchSize_Test, 1, 2])\n",
    "\n",
    "                    for ij in range(50):\n",
    "                        test_feed_dict = {Inputs:batch_Test_input, outputs:batch_Test_outputs}\n",
    "                        logits_Out = sess.run([logits], feed_dict = test_feed_dict)\n",
    "                        logits_Out = logits_Out[0]\n",
    "                        #print(logits_Out.shape)\n",
    "                        prediction = np.expand_dims(logits_Out[:,-1,:],axis = 1)\n",
    "                        batch_Test_outputs = np.hstack([batch_Test_outputs, prediction])\n",
    "                        \n",
    "                    loss_ = Loss_function_Test(logits_Out, batch_Test_targets) \n",
    "\n",
    "                    Test_loss.append(loss_)\n",
    "                    if jk%3 == 0:\n",
    "                        reDraw(batch_Test_targets.astype(int), logits_Out.astype(int),2, ' Testing')\n",
    "                \n",
    "                    \n",
    "                Test_loss = np.mean(Test_loss)\n",
    "                print('### TESTING TESTING TESING ###')\n",
    "                print('Epoch: '+str(i) + ' Step:' + str(k) + 'Test Loss is: => ' + str(Test_loss) + '\\n')\n",
    "                print('### TESTING TESTING TESING ###')\n",
    "                \n",
    "                \n",
    "            #if k%100 == 0:\n",
    "            #    reDraw(batch_targets.astype(int),logits_Out.astype(int), 3, ' Training')     \n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        if i%10 == 0:\n",
    "            saver.save(sess, './logs_dir_TELEGU_Error/' + \"model.ckpt\", i)  \n",
    "               \n",
    "    return random_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabelTrain_Targets, LabelTrain_Outputs = load_Coordinates('/media/ayan/Drive/Offline2Online/Tamil/telugu/')\n",
    "DataTrain = load_Images('/media/ayan/Drive/Offline2Online/Tamil/telugu/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "global sess\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session (config=config)\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_index = Compile(DataTrain,LabelTrain_Outputs, LabelTrain_Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "Input = tf.placeholder(tf.float32, shape = [32, 64, 64, 1], name = 'Inputs')\n",
    "with tf.variable_scope(\"Network\"):\n",
    "    with tf.variable_scope(\"CNN\"):\n",
    "        conv1 = tf.layers.conv2d(Input,64,3,activation=tf.nn.relu,name = 'conv1', padding = 'same')\n",
    "        pool1 = tf.layers.max_pooling2d(conv1,2,[2,2],name = 'pool1')\n",
    "            \n",
    "ckpt = tf.train.get_checkpoint_state('./logs_dir_TELEGU_Error/')\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"Model restored...\")\n",
    "#print(sess.run(tf.trainable_variables()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Test = DataTrain[random_index,:,:,:]\n",
    "LabelTest_Outputs = LabelTrain_Outputs[random_index, :,:]\n",
    "LabelTest_Targets = LabelTrain_Targets[random_index, :,:]\n",
    "\n",
    "\n",
    "Data_Train = np.delete(DataTrain,random_index, axis = 0)\n",
    "LabelTrain_Outputs = np.delete(LabelTrain_Outputs,random_index, axis = 0)\n",
    "LabelTrain_Targets = np.delete(LabelTrain_Targets,random_index, axis = 0)\n",
    "\n",
    "#[Total_Data, row, col, depth] = np.shape(Data_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(model_dir = './logs_dir_TELEGU_Error/'): \n",
    "    ckpt_file_path  = model_dir + [i for i in  os.listdir(model_dir) if i.endswith('meta')][0]\n",
    "    loader = tf.train.import_meta_graph(ckpt_file_path)\n",
    "    loader.restore(sess, tf.train.latest_checkpoint(model_dir))\n",
    "    graph = tf.get_default_graph()\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./logs_dir_TELEGU_Error/model.ckpt-40\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "global graph\n",
    "global sess\n",
    "sess = tf.Session ()\n",
    "graph= get_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LabelTrain_Targets, LabelTrain_Outputs = load_Coordinates('/media/ayan/Drive/Offline2Online/Tamil/telugu/')\n",
    "#Data_Train = load_Images('/media/ayan/Drive/Offline2Online/Tamil/telugu/')\n",
    "\n",
    "#[Total_Data, row, col, depth] = np.shape(Data_Train)\n",
    "\n",
    "\n",
    "#random_index = np.random.choice(Total_Data, 1000, replace=False)\n",
    "\n",
    "#Data_Test = Data_Train[random_index,:,:,:]\n",
    "#LabelTest_Outputs = LabelTrain_Outputs[random_index, :,:]\n",
    "#LabelTest_Targets = LabelTrain_Targets[random_index, :,:]\n",
    "\n",
    "\n",
    "#Data_Train = np.delete(Data_Train,random_index, axis = 0)\n",
    "#LabelTrain_Outputs = np.delete(LabelTrain_Outputs,random_index, axis = 0)\n",
    "#LabelTrain_Targets = np.delete(LabelTrain_Targets,random_index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Inputs = graph.get_tensor_by_name('Inputs:0')\n",
    "outputs = graph.get_tensor_by_name('outputs:0')\n",
    "targets = graph.get_tensor_by_name('targets:0')\n",
    "logits = graph.get_tensor_by_name('Network/Decoder/logits/BiasAdd:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.trainable_variables()\n",
    "kernel_0 = graph.get_tensor_by_name('Network/CNN/conv1/kernel:0')\n",
    "\n",
    "#init = tf.global_variables_initializer()\n",
    "#print(kernel_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[  3.09421346e-02  -3.80455144e-02   4.18581516e-02   7.33650476e-02\n",
      "     -4.45071831e-02  -5.49664684e-02  -1.36233032e-01   2.75193043e-02\n",
      "      1.61895797e-01  -8.27366672e-03  -1.78561747e-01  -3.49660479e-02\n",
      "      2.03686059e-02  -2.75989976e-02   1.18626125e-01  -9.90138799e-02\n",
      "     -3.68017815e-02   3.15786637e-02   3.83046381e-02  -5.38569223e-03\n",
      "      2.30972618e-02   1.40962191e-02   1.11561634e-01   1.05203629e-01\n",
      "      1.00139953e-01  -2.38342136e-02  -1.96031108e-01  -4.25074110e-03\n",
      "     -9.76578891e-02   1.13854064e-02  -1.44755065e-01   1.36243328e-01\n",
      "      1.49112895e-01   3.13988999e-02  -1.51007855e-02   1.36923641e-01\n",
      "      7.94806853e-02  -1.67758271e-01   1.32514611e-01  -4.27277982e-02\n",
      "     -4.01914120e-02  -5.22282422e-02  -1.72426164e-01   1.10759810e-01\n",
      "     -1.23551287e-01   3.88559997e-02  -7.62599707e-02  -1.35854140e-01\n",
      "      1.92850366e-01   1.89058259e-02   3.57168466e-02  -4.86660190e-02\n",
      "      9.40493867e-02  -3.91798234e-03  -1.36152387e-01   1.86873265e-02\n",
      "      8.33325312e-02  -1.54934004e-02   1.17328111e-02  -1.03247270e-01\n",
      "     -4.75866575e-04   1.14044867e-01   8.59212354e-02   1.16225511e-01]]\n",
      "\n",
      "  [[ -5.29298782e-02  -6.07574312e-03  -4.41965684e-02  -5.13766706e-03\n",
      "     -1.04327209e-01  -6.31195605e-02  -8.66043568e-03  -2.43619699e-02\n",
      "     -1.36089608e-01  -1.13464780e-02   1.40095398e-01  -1.90709591e-01\n",
      "     -7.66247604e-03  -1.52042642e-01  -4.40990105e-02  -6.09807298e-02\n",
      "      1.36324450e-01  -6.59175636e-03  -2.67716497e-03  -3.69395316e-01\n",
      "      2.68146753e-01  -4.47328016e-03  -8.06555990e-03   8.50481214e-04\n",
      "      5.53095788e-02  -3.92535478e-02  -2.92677116e-02   6.19180799e-02\n",
      "      5.36511391e-02  -1.48609923e-02  -1.04339920e-01   1.01076506e-01\n",
      "      1.17575554e-02   7.80719444e-02   2.99984310e-02   2.73879189e-02\n",
      "      4.82997112e-02  -2.21873194e-01   1.17070831e-01   7.49318022e-03\n",
      "     -1.35968462e-01  -6.21983707e-02  -8.50229114e-02  -1.24305114e-02\n",
      "      8.12215805e-02  -2.77047399e-02   1.04756705e-01  -4.28957492e-02\n",
      "     -4.68436256e-03  -5.52848540e-02  -7.63416439e-02  -1.09986052e-01\n",
      "     -3.07343416e-02   8.32103938e-02   1.31976709e-01   2.40830984e-02\n",
      "     -1.98871107e-03  -2.44473591e-02   2.41941046e-02   7.67240748e-02\n",
      "      3.24980468e-02   1.03354096e-01  -1.30684584e-01   4.83763218e-03]]\n",
      "\n",
      "  [[ -9.55983996e-02  -2.60566603e-02  -3.38880671e-03   3.44978981e-02\n",
      "      1.17837153e-01  -7.56070018e-04   1.24513455e-01   6.94479346e-02\n",
      "     -1.87676355e-01   2.91706901e-02  -1.67284429e-01   7.79511500e-03\n",
      "     -2.85023041e-02  -4.89877798e-02   9.02616829e-02  -5.91235757e-02\n",
      "     -1.76484630e-01  -4.47456203e-02   1.26431268e-02   7.54782408e-02\n",
      "      3.74390706e-02  -4.16006707e-02  -1.84411243e-01   3.56849097e-02\n",
      "      5.20552136e-03  -1.86761141e-01  -1.41730485e-02  -3.59394997e-02\n",
      "     -5.69964387e-02   8.24021001e-04  -1.01847082e-01   1.45988241e-01\n",
      "      4.09769006e-02   9.20546129e-02  -4.15311269e-02   7.62101114e-02\n",
      "     -8.59295055e-02  -2.69763678e-01   1.45290464e-01  -7.40296533e-03\n",
      "      2.96817608e-02  -8.22244287e-02   7.98681453e-02   3.80568840e-02\n",
      "     -5.20416461e-02   1.04817219e-01  -1.05651215e-01   1.17183320e-01\n",
      "      1.40340731e-01   2.67425664e-02  -1.07877525e-02  -2.08682075e-01\n",
      "     -3.20213884e-02   3.15049626e-02  -1.37757719e-01  -2.80134697e-02\n",
      "      8.06853361e-03  -6.48547113e-02  -4.67489362e-02  -7.71980882e-02\n",
      "     -2.78087100e-03  -1.24887742e-01  -2.19307058e-02   4.23761643e-02]]]\n",
      "\n",
      "\n",
      " [[[ -3.79781798e-02   9.17295553e-03   1.07632522e-02   1.17742479e-01\n",
      "     -2.26630606e-02  -9.85091254e-02  -1.78623989e-01   1.39414579e-01\n",
      "     -3.58772427e-01  -1.01065233e-01  -7.00104013e-02  -3.66772152e-02\n",
      "      1.68722998e-02  -1.15732118e-01   1.07453898e-01   8.22625011e-02\n",
      "     -1.70864910e-01   1.48212343e-01   8.81984383e-02  -3.72777164e-01\n",
      "      7.07832128e-02   1.53835684e-01  -2.69269254e-02   1.57898113e-01\n",
      "     -5.99344680e-03  -1.59588590e-01   4.96752858e-02  -2.30412260e-02\n",
      "      3.55559438e-02  -5.23524266e-03  -1.81297243e-01  -1.90112621e-01\n",
      "      1.41313776e-01   1.50927290e-01   4.35200110e-02  -3.00663989e-02\n",
      "      1.58509284e-01  -1.97603300e-01   1.96653251e-02   9.46783647e-03\n",
      "     -1.11214772e-01   2.75687873e-02   2.25477964e-01   3.40795964e-02\n",
      "      1.04200300e-02   5.01773320e-02   6.00562617e-03  -3.10476840e-01\n",
      "     -1.79693297e-01  -1.23679582e-02  -1.17348254e-01   2.07051262e-01\n",
      "     -5.09740114e-02   9.03715286e-03  -6.32376671e-02   5.50599918e-02\n",
      "      1.92020722e-02  -4.13162187e-02   2.73706466e-02   1.59157410e-01\n",
      "      8.88351426e-02   1.99513584e-02   3.83201092e-02   1.66728720e-01]]\n",
      "\n",
      "  [[  8.07771534e-02   1.60990447e-01  -2.84101479e-02   1.14814201e-02\n",
      "     -1.25082150e-01  -1.36699304e-02   2.66088136e-02  -8.15340802e-02\n",
      "     -6.17694370e-02   9.21128169e-02  -5.60917705e-02   5.03240749e-02\n",
      "      1.84354037e-02   1.06510691e-01  -7.73099661e-02   8.56860261e-03\n",
      "     -1.46465758e-02   4.80421335e-02  -5.56613989e-02  -6.00616001e-02\n",
      "      2.34519556e-01  -6.78885670e-04   7.56928325e-02   2.96435393e-02\n",
      "      1.59310237e-01   1.81520134e-02  -1.89125594e-02   6.13633655e-02\n",
      "      1.98075473e-01   1.07876815e-01  -1.70329809e-01  -8.01003948e-02\n",
      "      8.02470185e-03   1.09415337e-01  -5.29146455e-02  -1.23831583e-02\n",
      "      7.15008378e-02  -2.54569381e-01  -1.48070723e-01  -2.68233325e-02\n",
      "      1.00441873e-01   6.07573651e-02   8.35926160e-02   8.16488042e-02\n",
      "      2.52947271e-01   2.06622556e-02  -6.64387643e-02   8.87505561e-02\n",
      "     -1.03959873e-01  -1.51308486e-02  -2.33097672e-01   1.06771886e-01\n",
      "      2.46438980e-02   1.27707899e-01  -7.38285854e-02  -3.67108807e-02\n",
      "      5.18865995e-02  -8.41010213e-02   1.12453081e-01   1.31095827e-01\n",
      "     -1.15558684e-01   1.08098984e-01   3.88779119e-02   3.50112282e-02]]\n",
      "\n",
      "  [[  5.02584018e-02   3.96576524e-02   2.70297248e-02   3.37131973e-03\n",
      "      1.39304921e-01  -4.91618924e-02   8.73731896e-02   2.92298459e-02\n",
      "     -1.05398975e-01  -1.12487763e-01  -6.87480345e-02   6.87203482e-02\n",
      "     -4.38284827e-03   1.28396498e-02   6.61759898e-02   8.88122991e-02\n",
      "     -7.17225671e-02   4.56287935e-02   8.92176572e-03   9.93372947e-02\n",
      "      6.06938638e-02   4.38730195e-02  -1.63908284e-02   1.47187430e-02\n",
      "     -4.18803655e-02   4.08037826e-02   9.29922685e-02  -6.85568824e-02\n",
      "      6.16920218e-02   3.89771559e-03  -1.37078822e-01  -2.53862143e-01\n",
      "      3.07167713e-02   7.00636432e-02   1.43719958e-02   1.93376616e-02\n",
      "     -1.04598500e-01  -4.70838137e-02  -1.54564992e-01   7.54859895e-02\n",
      "     -1.01874784e-01  -1.42802715e-01  -2.02050731e-02  -1.21955819e-01\n",
      "      2.19500028e-02  -1.03546688e-02  -3.23104933e-02   8.34291615e-03\n",
      "      4.70620878e-02   9.02220383e-02   1.30911082e-01   2.92024221e-02\n",
      "     -1.58947572e-01  -5.43244071e-02  -6.66809231e-02  -1.05754361e-01\n",
      "     -1.10477833e-02  -6.79350644e-03  -1.69790894e-01  -4.62612472e-02\n",
      "      8.17457065e-02  -1.27503872e-01  -6.71462864e-02   1.50859421e-02]]]\n",
      "\n",
      "\n",
      " [[[  4.35283817e-02  -2.86901239e-02   1.70183815e-02   2.06242561e-01\n",
      "      5.13218716e-03  -5.16797453e-02   9.74495057e-03   1.71611950e-01\n",
      "     -1.99354231e-01  -6.25332026e-03  -1.72528505e-01   1.29866570e-01\n",
      "      2.37920254e-01   2.47804523e-02   7.48891383e-02   9.51451063e-02\n",
      "     -1.62263766e-01   1.04987524e-01   1.70737043e-01  -2.14962497e-01\n",
      "     -1.57137830e-02   1.35025054e-01  -6.46289587e-02   8.07765350e-02\n",
      "      7.76910130e-03  -1.51308462e-01   1.34405956e-01   2.36490816e-02\n",
      "     -8.19630362e-03   9.03415754e-02   5.61032066e-05  -1.39758080e-01\n",
      "     -1.78545229e-02  -3.51399541e-01   6.84981793e-02  -1.11100208e-02\n",
      "     -3.71203502e-03   1.59504429e-01  -2.44758070e-01   1.15626045e-01\n",
      "      2.24991515e-03  -2.18504015e-03  -9.21890140e-02   9.87742003e-03\n",
      "     -4.52891961e-02   1.70028225e-01   6.94612488e-02  -3.74660678e-02\n",
      "     -6.32829145e-02   2.46449257e-03  -1.99502364e-01  -1.52316615e-01\n",
      "     -4.55121994e-02  -7.14384615e-02  -1.60879139e-02   7.26386383e-02\n",
      "     -4.28103916e-02  -6.05145916e-02  -4.94782701e-02   1.43881530e-01\n",
      "     -1.26741365e-01   1.16847074e-02   4.82427934e-03   5.12529304e-03]]\n",
      "\n",
      "  [[ -5.63031249e-02   2.64801711e-01   1.12630635e-01  -4.64736950e-04\n",
      "      1.03215892e-02  -1.00577652e-01   3.89009528e-02   8.97499174e-03\n",
      "      1.66113406e-01   5.58546335e-02   6.91716820e-02  -3.72515731e-02\n",
      "     -4.39887121e-03   1.17941812e-01  -2.83135951e-01  -1.23657938e-02\n",
      "      6.62449300e-02   1.06055871e-01   5.24261668e-02   2.15294417e-02\n",
      "      7.22046196e-02  -1.36372203e-03   2.16711164e-02   7.50020903e-04\n",
      "      2.95728426e-02  -9.14267898e-02  -1.15142338e-01   1.87858418e-01\n",
      "      2.27077618e-01   2.10340932e-01   1.15079284e-01   3.43242884e-02\n",
      "     -4.29297499e-02  -1.42426491e-01  -2.42686132e-03  -2.82502454e-02\n",
      "      2.90989634e-02  -2.98115853e-02  -1.02481492e-01   4.38383594e-02\n",
      "      7.89065957e-02  -2.26057366e-01  -7.36445859e-02   1.07622854e-01\n",
      "      8.66419747e-02   4.25588824e-02   1.31425187e-02  -2.94449460e-02\n",
      "     -9.93627831e-02   1.12039655e-01  -3.53829823e-02  -1.47254700e-02\n",
      "      1.36649579e-01   2.76220366e-02   2.78062504e-02  -9.96488146e-04\n",
      "     -5.87094226e-04  -4.70188744e-02   1.26255034e-02   2.55965404e-02\n",
      "     -9.25161242e-02   1.07901804e-01  -4.35371399e-02  -2.34581158e-02]]\n",
      "\n",
      "  [[  8.21643472e-02  -2.37512141e-02  -5.64821027e-02   8.10039043e-02\n",
      "      7.31416792e-02  -7.10509792e-02   1.67504717e-02   3.08889467e-02\n",
      "     -1.53422967e-01  -1.26401591e-03  -1.30412936e-01   2.95169931e-02\n",
      "      6.68287054e-02  -3.81233878e-02  -8.34851414e-02  -1.04267828e-01\n",
      "     -1.90838456e-01  -8.71470105e-03   7.00922385e-02   1.99463412e-01\n",
      "     -7.16347247e-02   1.06936187e-01   9.04347654e-03   1.14064924e-01\n",
      "     -1.98064372e-02   2.15819046e-01  -2.01618969e-01  -5.35928681e-02\n",
      "      4.23955023e-02  -5.44969179e-03   5.02923839e-02   6.77423999e-02\n",
      "      5.09786084e-02  -2.13268295e-01   4.42518592e-02   7.86173623e-03\n",
      "     -2.98742000e-02   1.35566399e-01  -1.87163260e-02  -5.80052100e-03\n",
      "      7.62953982e-02   2.25955635e-01   5.60171567e-02  -1.19995391e-02\n",
      "      1.59357786e-02   3.36076654e-02   5.40523119e-02   1.33682787e-01\n",
      "      5.03277988e-04   4.64319997e-02   1.43444255e-01   1.73418775e-01\n",
      "     -4.11056243e-02  -1.90200537e-01  -3.43177952e-02   4.62244153e-02\n",
      "      6.10740529e-03  -9.45821330e-02  -5.42139374e-02  -8.82160217e-02\n",
      "      1.13242753e-01   8.22399370e-03   9.94066969e-02   1.96041875e-02]]]]\n"
     ]
    }
   ],
   "source": [
    "BB = sess.run(kernel_0)\n",
    "print(BB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = tf.placeholder(tf.float32, shape = [32, 64, 64, 1], name = 'Inputs')\n",
    "with tf.variable_scope(\"Network\"):\n",
    "    with tf.variable_scope(\"CNN\"):\n",
    "        k_init = tf.constant_initializer(BB)\n",
    "        conv1 = tf.layers.conv2d(Input,64,3,activation=tf.nn.relu,name = 'conv1', kernel_initializer = k_init, padding = 'same')\n",
    "        pool1 = tf.layers.max_pooling2d(conv1,2,[2,2],name = 'pool1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Network/CNN/conv1/kernel:0' shape=(3, 3, 1, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'Network/CNN/conv1/bias:0' shape=(64,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True]]],\n",
       "\n",
       "\n",
       "       [[[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True]]],\n",
       "\n",
       "\n",
       "       [[[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True]]]], dtype=bool)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "CC = sess.run(tf.trainable_variables()[0])\n",
    "BB == CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.layers.conv2d(kernel_initializer = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch_Test_input[0,:,:,0])\n",
    "plt.show()\n",
    "BB = batch_Test_input[0,:,:,0]\n",
    "BB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(BB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileid = open('testfile.txt','w') \n",
    "fileid.write(batch_Test_targets[0,:,:])\n",
    "fileid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_Test_targets[0,:,:]\n",
    "np.savetxt('testfile2.txt', batch_Test_targets[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_loss = []\n",
    "BatchSize = 32\n",
    "BatchSize_Test = BatchSize\n",
    "Save_folder = '/media/ayan/Drive/Offline2Online/Final_results_TELEGU_Error/'\n",
    "\n",
    "number = 0\n",
    "for jk in range(1000//BatchSize_Test):    \n",
    "    \n",
    "    batch_Test_input = Data_Test[jk*BatchSize_Test:(jk+1)*BatchSize_Test,:,:,:]  \n",
    "    batch_Test_targets = LabelTest_Targets[jk*BatchSize_Test:(jk+1)*BatchSize_Test,:,:]                         \n",
    "    batch_Test_outputs =  np.zeros(shape = [BatchSize_Test, 1, 2])\n",
    "    \n",
    "    for ij in range(50):\n",
    "        test_feed_dict = {Inputs:batch_Test_input, outputs:batch_Test_outputs}\n",
    "        logits_Out = sess.run([logits], feed_dict = test_feed_dict)\n",
    "        logits_Out = logits_Out[0]\n",
    "        #print(logits_Out.shape)\n",
    "        prediction = np.expand_dims(logits_Out[:,-1,:],axis = 1)\n",
    "        batch_Test_outputs = np.hstack([batch_Test_outputs, prediction])\n",
    "\n",
    "    loss_ = Loss_function_Test(logits_Out, batch_Test_targets) \n",
    "\n",
    "    Test_loss.append(loss_)\n",
    "    \n",
    "    for num in range(32):\n",
    "        directory = Save_folder + str(number) + '/'\n",
    "        try:\n",
    "            os.stat(directory)\n",
    "        except:\n",
    "            os.mkdir(directory) \n",
    "        img = 255 - batch_Test_input[num, :,:,0]\n",
    "        imgpath = directory + str(number) + '.jpg'\n",
    "        cv2.imwrite(imgpath, img)\n",
    "        np.savetxt(  directory + 'Grountruth_' + str(num) + '.txt', batch_Test_targets[num,:,:])\n",
    "        np.savetxt(  directory + 'Predict_' + str(num) + '.txt', logits_Out[num,:,:])\n",
    "        number = number + 1\n",
    "        \n",
    "    if jk%3 == 0:\n",
    "        reDraw(batch_Test_targets.astype(int), logits_Out.astype(int),2, ' Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
